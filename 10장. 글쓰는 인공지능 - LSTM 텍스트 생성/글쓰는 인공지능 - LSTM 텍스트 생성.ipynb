{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2b67d4d-ed2a-4f18-a2f3-cc5ef2a3aa54",
   "metadata": {},
   "source": [
    "# 데이터 살펴보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ad96eef-1f4e-43f9-b3fe-dacf0ba38fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63e33567-a9f7-4e29-ae5f-b07a57fc44a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['abstract', 'articleID', 'articleWordCount', 'byline', 'documentType',\n",
       "       'headline', 'keywords', 'multimedia', 'newDesk', 'printPage', 'pubDate',\n",
       "       'sectionName', 'snippet', 'source', 'typeOfMaterial', 'webURL'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../input/New York Times Comments/ArticlesApril2017.csv\")\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbde014-920b-49a1-9530-08f11abdcf36",
   "metadata": {},
   "source": [
    "사람이 직접 작성한 기사가 들어있는 headline만 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3092255c-cd6d-4547-8a87-4c9e5910cd39",
   "metadata": {},
   "source": [
    "# 학습용 데이터 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfac2a94-f995-4226-8442-5246835f00a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data.dataset import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f111d31-5785-4c82-965a-b0e12a2347bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGeneration(Dataset):\n",
    "    def clean_text(self, txt):\n",
    "        txt = \"\".join(v for v in txt if v not in string.punctuation).lower()\n",
    "        return txt\n",
    "\n",
    "    def __init__(self):\n",
    "        all_headlines = []\n",
    "\n",
    "        for filename in glob.glob(\"../input/New York Times Comments/*.csv\"):\n",
    "            if \"Articles\" in filename:\n",
    "                article_df = pd.read_csv(filename)\n",
    "\n",
    "                all_headlines.extend(list(article_df.headline.values))\n",
    "                break\n",
    "\n",
    "        all_headlines = [h for h in all_headlines if h != \"Unknown\"]\n",
    "\n",
    "        self.corpus = [self.clean_text(x) for x in all_headlines]\n",
    "        self.BOW = {}\n",
    "\n",
    "        for line in self.corpus:\n",
    "            for word in line.split():\n",
    "                if word not in self.BOW.keys():\n",
    "                    self.BOW[word] = len(self.BOW.keys())\n",
    "\n",
    "        self.data = self.generate_sequence(self.corpus)\n",
    "\n",
    "    def generate_sequence(self, txt):\n",
    "        seq = []\n",
    "        for line in txt:\n",
    "            line = line.split()\n",
    "            line_bow = [self.BOW[word] for word in line]\n",
    "\n",
    "            # 단어 2개를 입력으로, 그 다음 단어를 정답으로\n",
    "            data = [\n",
    "                ([line_bow[i], line_bow[i + 1]], line_bow[i + 2])\n",
    "                for i in range(len(line_bow) - 2)\n",
    "            ]\n",
    "\n",
    "            seq.extend(data)\n",
    "\n",
    "        return seq\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        data = np.array(self.data[i][0])\n",
    "        label = np.array(self.data[i][1]).astype(np.float32)\n",
    "\n",
    "        return data, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4876ed-8924-44b9-ad3c-9b2b492ef095",
   "metadata": {},
   "source": [
    "# LSTM 모델 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "addf1cbb-ecf6-4ae8-919f-9264a4ba074e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd78b9c0-3fd8-4a18-b233-6571faf963b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, num_embeddings):\n",
    "        super(LSTM, self).__init__()\n",
    "\n",
    "        self.embed = nn.Embedding(num_embeddings=num_embeddings, embedding_dim=16)\n",
    "\n",
    "        # LSTM 층의 출력은 (batch_size, sequence_length, hidden_size)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=16, hidden_size=64, num_layers=5, batch_first=True\n",
    "        )\n",
    "\n",
    "        self.fc1 = nn.Linear(128, num_embeddings)\n",
    "        self.fc2 = nn.Linear(num_embeddings, num_embeddings)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "\n",
    "        x, _ = self.lstm(x)\n",
    "        x = torch.reshape(x, (x.shape[0], -1))\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a4e14e-a8f1-439a-9eee-a0a4502ecf4d",
   "metadata": {},
   "source": [
    "# 학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7a59da1-5725-4b9f-b611-614e28208dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tqdm\n",
    "from torch.optim.adam import Adam\n",
    "from torch.utils.data.dataloader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92aca082-b4be-40c3-bc7c-07c2573c8a99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78b71b12-a849-4864-ac77-3d912cb5bfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TextGeneration()\n",
    "model = LSTM(num_embeddings=len(dataset.BOW)).to(device)\n",
    "loader = DataLoader(dataset, batch_size=64)\n",
    "optim = Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69180911-5d49-42d8-8e52-d9a238bc2a2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/63 [00:00<?, ?it/s]C:\\Users\\jiweo\\AppData\\Local\\Temp\\ipykernel_41808\\2812280455.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  pred = model(torch.tensor(data, dtype=torch.long).to(device))\n",
      "C:\\Users\\jiweo\\AppData\\Local\\Temp\\ipykernel_41808\\2812280455.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  pred, torch.tensor(label, dtype=torch.long).to(device)\n",
      "epoch 0 loss: 7.37069845199585: 100%|██████████████████████████████████████████████████| 63/63 [00:00<00:00, 75.57it/s]\n",
      "epoch 1 loss: 7.011480808258057: 100%|████████████████████████████████████████████████| 63/63 [00:00<00:00, 138.30it/s]\n",
      "epoch 2 loss: 6.760417938232422: 100%|████████████████████████████████████████████████| 63/63 [00:00<00:00, 134.30it/s]\n",
      "epoch 3 loss: 6.566870212554932: 100%|████████████████████████████████████████████████| 63/63 [00:00<00:00, 136.29it/s]\n",
      "epoch 4 loss: 6.428116798400879: 100%|████████████████████████████████████████████████| 63/63 [00:00<00:00, 134.19it/s]\n",
      "epoch 5 loss: 6.278395652770996: 100%|████████████████████████████████████████████████| 63/63 [00:00<00:00, 138.47it/s]\n",
      "epoch 6 loss: 6.092379570007324: 100%|████████████████████████████████████████████████| 63/63 [00:00<00:00, 136.88it/s]\n",
      "epoch 7 loss: 5.989106178283691: 100%|████████████████████████████████████████████████| 63/63 [00:00<00:00, 136.99it/s]\n",
      "epoch 8 loss: 5.74649715423584: 100%|█████████████████████████████████████████████████| 63/63 [00:00<00:00, 134.46it/s]\n",
      "epoch 9 loss: 5.734908103942871: 100%|████████████████████████████████████████████████| 63/63 [00:00<00:00, 137.24it/s]\n",
      "epoch 10 loss: 5.661248683929443: 100%|███████████████████████████████████████████████| 63/63 [00:00<00:00, 134.62it/s]\n",
      "epoch 11 loss: 5.727349281311035: 100%|███████████████████████████████████████████████| 63/63 [00:00<00:00, 137.28it/s]\n",
      "epoch 12 loss: 5.953260898590088: 100%|███████████████████████████████████████████████| 63/63 [00:00<00:00, 137.88it/s]\n",
      "epoch 13 loss: 6.0288896560668945: 100%|██████████████████████████████████████████████| 63/63 [00:00<00:00, 136.61it/s]\n",
      "epoch 14 loss: 5.816989421844482: 100%|███████████████████████████████████████████████| 63/63 [00:00<00:00, 134.41it/s]\n",
      "epoch 15 loss: 5.588061332702637: 100%|███████████████████████████████████████████████| 63/63 [00:00<00:00, 132.68it/s]\n",
      "epoch 16 loss: 5.42132568359375: 100%|████████████████████████████████████████████████| 63/63 [00:00<00:00, 134.62it/s]\n",
      "epoch 17 loss: 5.274007320404053: 100%|███████████████████████████████████████████████| 63/63 [00:00<00:00, 135.29it/s]\n",
      "epoch 18 loss: 5.451775550842285: 100%|███████████████████████████████████████████████| 63/63 [00:00<00:00, 133.70it/s]\n",
      "epoch 19 loss: 5.175243377685547: 100%|███████████████████████████████████████████████| 63/63 [00:00<00:00, 135.97it/s]\n",
      "epoch 20 loss: 5.237630367279053: 100%|███████████████████████████████████████████████| 63/63 [00:00<00:00, 130.50it/s]\n",
      "epoch 21 loss: 5.043996810913086: 100%|███████████████████████████████████████████████| 63/63 [00:00<00:00, 132.62it/s]\n",
      "epoch 22 loss: 4.9045844078063965: 100%|██████████████████████████████████████████████| 63/63 [00:00<00:00, 136.26it/s]\n",
      "epoch 23 loss: 4.729404449462891: 100%|███████████████████████████████████████████████| 63/63 [00:00<00:00, 132.60it/s]\n",
      "epoch 24 loss: 4.605177879333496: 100%|███████████████████████████████████████████████| 63/63 [00:00<00:00, 132.91it/s]\n",
      "epoch 25 loss: 4.429410457611084: 100%|███████████████████████████████████████████████| 63/63 [00:00<00:00, 132.75it/s]\n",
      "epoch 26 loss: 4.284933090209961: 100%|███████████████████████████████████████████████| 63/63 [00:00<00:00, 128.70it/s]\n",
      "epoch 27 loss: 4.213940620422363: 100%|███████████████████████████████████████████████| 63/63 [00:00<00:00, 134.33it/s]\n",
      "epoch 28 loss: 4.2053632736206055: 100%|██████████████████████████████████████████████| 63/63 [00:00<00:00, 130.78it/s]\n",
      "epoch 29 loss: 4.174464702606201: 100%|███████████████████████████████████████████████| 63/63 [00:00<00:00, 135.35it/s]\n",
      "epoch 30 loss: 4.122637748718262: 100%|███████████████████████████████████████████████| 63/63 [00:00<00:00, 134.40it/s]\n",
      "epoch 31 loss: 4.180130481719971: 100%|███████████████████████████████████████████████| 63/63 [00:00<00:00, 131.63it/s]\n",
      "epoch 32 loss: 4.269852638244629: 100%|███████████████████████████████████████████████| 63/63 [00:00<00:00, 131.04it/s]\n",
      "epoch 33 loss: 4.299511432647705: 100%|███████████████████████████████████████████████| 63/63 [00:00<00:00, 131.25it/s]\n",
      "epoch 34 loss: 4.298608779907227: 100%|███████████████████████████████████████████████| 63/63 [00:00<00:00, 133.31it/s]\n",
      "epoch 35 loss: 4.024299144744873: 100%|███████████████████████████████████████████████| 63/63 [00:00<00:00, 132.16it/s]\n",
      "epoch 36 loss: 3.9681777954101562: 100%|██████████████████████████████████████████████| 63/63 [00:00<00:00, 134.44it/s]\n",
      "epoch 37 loss: 3.995344877243042: 100%|███████████████████████████████████████████████| 63/63 [00:00<00:00, 127.69it/s]\n",
      "epoch 38 loss: 4.194449424743652: 100%|███████████████████████████████████████████████| 63/63 [00:00<00:00, 135.92it/s]\n",
      "epoch 39 loss: 4.098062515258789: 100%|███████████████████████████████████████████████| 63/63 [00:00<00:00, 131.44it/s]\n",
      "epoch 40 loss: 4.149252414703369: 100%|███████████████████████████████████████████████| 63/63 [00:00<00:00, 132.95it/s]\n",
      "epoch 41 loss: 4.026791572570801: 100%|███████████████████████████████████████████████| 63/63 [00:00<00:00, 133.57it/s]\n",
      "epoch 42 loss: 3.946382999420166: 100%|███████████████████████████████████████████████| 63/63 [00:00<00:00, 127.84it/s]\n",
      "epoch 43 loss: 3.9336440563201904: 100%|██████████████████████████████████████████████| 63/63 [00:00<00:00, 133.39it/s]\n",
      "epoch 44 loss: 3.94860577583313: 100%|████████████████████████████████████████████████| 63/63 [00:00<00:00, 131.92it/s]\n",
      "epoch 45 loss: 4.017626762390137: 100%|███████████████████████████████████████████████| 63/63 [00:00<00:00, 133.26it/s]\n",
      "epoch 46 loss: 3.816668748855591: 100%|███████████████████████████████████████████████| 63/63 [00:00<00:00, 130.11it/s]\n",
      "epoch 47 loss: 3.7438464164733887: 100%|██████████████████████████████████████████████| 63/63 [00:00<00:00, 130.15it/s]\n",
      "epoch 48 loss: 3.7052550315856934: 100%|██████████████████████████████████████████████| 63/63 [00:00<00:00, 133.61it/s]\n",
      "epoch 49 loss: 3.5105533599853516: 100%|██████████████████████████████████████████████| 63/63 [00:00<00:00, 129.69it/s]\n",
      "epoch 50 loss: 3.375959873199463: 100%|███████████████████████████████████████████████| 63/63 [00:00<00:00, 134.11it/s]\n",
      "epoch 51 loss: 3.2834479808807373: 100%|██████████████████████████████████████████████| 63/63 [00:00<00:00, 135.84it/s]\n",
      "epoch 52 loss: 3.3131542205810547: 100%|██████████████████████████████████████████████| 63/63 [00:00<00:00, 134.18it/s]\n",
      "epoch 53 loss: 3.298511505126953: 100%|███████████████████████████████████████████████| 63/63 [00:00<00:00, 130.69it/s]\n",
      "epoch 54 loss: 3.278782606124878: 100%|███████████████████████████████████████████████| 63/63 [00:00<00:00, 134.62it/s]\n",
      "epoch 55 loss: 3.3215038776397705: 100%|██████████████████████████████████████████████| 63/63 [00:00<00:00, 132.35it/s]\n",
      "epoch 56 loss: 3.2731010913848877: 100%|██████████████████████████████████████████████| 63/63 [00:00<00:00, 131.08it/s]\n",
      "epoch 57 loss: 3.3535380363464355: 100%|██████████████████████████████████████████████| 63/63 [00:00<00:00, 133.59it/s]\n",
      "epoch 58 loss: 3.55009388923645: 100%|████████████████████████████████████████████████| 63/63 [00:00<00:00, 131.40it/s]\n",
      "epoch 59 loss: 3.498156785964966: 100%|███████████████████████████████████████████████| 63/63 [00:00<00:00, 134.97it/s]\n",
      "epoch 60 loss: 3.3312835693359375: 100%|██████████████████████████████████████████████| 63/63 [00:00<00:00, 133.79it/s]\n",
      "epoch 61 loss: 3.1896438598632812: 100%|██████████████████████████████████████████████| 63/63 [00:00<00:00, 138.17it/s]\n",
      "epoch 62 loss: 3.063197612762451: 100%|███████████████████████████████████████████████| 63/63 [00:00<00:00, 128.83it/s]\n",
      "epoch 63 loss: 3.014277219772339: 100%|███████████████████████████████████████████████| 63/63 [00:00<00:00, 134.41it/s]\n",
      "epoch 64 loss: 2.9090206623077393: 100%|██████████████████████████████████████████████| 63/63 [00:00<00:00, 131.11it/s]\n",
      "epoch 65 loss: 2.8886170387268066: 100%|██████████████████████████████████████████████| 63/63 [00:00<00:00, 130.89it/s]\n",
      "epoch 66 loss: 2.859999179840088: 100%|███████████████████████████████████████████████| 63/63 [00:00<00:00, 131.69it/s]\n",
      "epoch 67 loss: 2.9063844680786133: 100%|██████████████████████████████████████████████| 63/63 [00:00<00:00, 130.56it/s]\n",
      "epoch 68 loss: 3.0957720279693604: 100%|██████████████████████████████████████████████| 63/63 [00:00<00:00, 132.82it/s]\n",
      "epoch 69 loss: 3.1944451332092285: 100%|██████████████████████████████████████████████| 63/63 [00:00<00:00, 134.13it/s]\n",
      "epoch 70 loss: 3.229869842529297: 100%|███████████████████████████████████████████████| 63/63 [00:00<00:00, 134.91it/s]\n",
      "epoch 71 loss: 3.1837949752807617: 100%|██████████████████████████████████████████████| 63/63 [00:00<00:00, 129.37it/s]\n",
      "epoch 72 loss: 2.9549577236175537: 100%|██████████████████████████████████████████████| 63/63 [00:00<00:00, 134.84it/s]\n",
      "epoch 73 loss: 2.809218645095825: 100%|███████████████████████████████████████████████| 63/63 [00:00<00:00, 135.86it/s]\n",
      "epoch 74 loss: 2.7533798217773438: 100%|██████████████████████████████████████████████| 63/63 [00:00<00:00, 129.45it/s]\n",
      "epoch 75 loss: 2.679619550704956: 100%|███████████████████████████████████████████████| 63/63 [00:00<00:00, 134.18it/s]\n",
      "epoch 76 loss: 2.7043533325195312: 100%|██████████████████████████████████████████████| 63/63 [00:00<00:00, 135.36it/s]\n",
      "epoch 77 loss: 2.632697820663452: 100%|███████████████████████████████████████████████| 63/63 [00:00<00:00, 130.51it/s]\n",
      "epoch 78 loss: 2.516864776611328: 100%|███████████████████████████████████████████████| 63/63 [00:00<00:00, 132.22it/s]\n",
      "epoch 79 loss: 2.529839515686035: 100%|███████████████████████████████████████████████| 63/63 [00:00<00:00, 136.40it/s]\n",
      "epoch 80 loss: 2.432896852493286: 100%|███████████████████████████████████████████████| 63/63 [00:00<00:00, 131.02it/s]\n",
      "epoch 81 loss: 2.385002374649048: 100%|███████████████████████████████████████████████| 63/63 [00:00<00:00, 135.25it/s]\n",
      "epoch 82 loss: 2.4855830669403076: 100%|██████████████████████████████████████████████| 63/63 [00:00<00:00, 130.47it/s]\n",
      "epoch 83 loss: 2.341386318206787: 100%|███████████████████████████████████████████████| 63/63 [00:00<00:00, 133.82it/s]\n",
      "epoch 84 loss: 2.375887393951416: 100%|███████████████████████████████████████████████| 63/63 [00:00<00:00, 133.39it/s]\n",
      "epoch 85 loss: 2.3934102058410645: 100%|██████████████████████████████████████████████| 63/63 [00:00<00:00, 129.42it/s]\n",
      "epoch 86 loss: 2.426058530807495: 100%|███████████████████████████████████████████████| 63/63 [00:00<00:00, 136.75it/s]\n",
      "epoch 87 loss: 2.428942918777466: 100%|███████████████████████████████████████████████| 63/63 [00:00<00:00, 133.47it/s]\n",
      "epoch 88 loss: 2.298332929611206: 100%|███████████████████████████████████████████████| 63/63 [00:00<00:00, 120.61it/s]\n",
      "epoch 89 loss: 2.3151674270629883: 100%|██████████████████████████████████████████████| 63/63 [00:00<00:00, 126.77it/s]\n",
      "epoch 90 loss: 2.2126944065093994: 100%|██████████████████████████████████████████████| 63/63 [00:00<00:00, 129.43it/s]\n",
      "epoch 91 loss: 2.0993781089782715: 100%|██████████████████████████████████████████████| 63/63 [00:00<00:00, 128.26it/s]\n",
      "epoch 92 loss: 2.4117021560668945: 100%|██████████████████████████████████████████████| 63/63 [00:00<00:00, 132.29it/s]\n",
      "epoch 93 loss: 2.4054758548736572: 100%|██████████████████████████████████████████████| 63/63 [00:00<00:00, 127.63it/s]\n",
      "epoch 94 loss: 2.560469150543213: 100%|███████████████████████████████████████████████| 63/63 [00:00<00:00, 134.11it/s]\n",
      "epoch 95 loss: 2.5243828296661377: 100%|██████████████████████████████████████████████| 63/63 [00:00<00:00, 127.60it/s]\n",
      "epoch 96 loss: 2.369537830352783: 100%|███████████████████████████████████████████████| 63/63 [00:00<00:00, 134.23it/s]\n",
      "epoch 97 loss: 2.220695972442627: 100%|███████████████████████████████████████████████| 63/63 [00:00<00:00, 131.24it/s]\n",
      "epoch 98 loss: 2.223987340927124: 100%|███████████████████████████████████████████████| 63/63 [00:00<00:00, 134.99it/s]\n",
      "epoch 99 loss: 2.535212993621826: 100%|███████████████████████████████████████████████| 63/63 [00:00<00:00, 130.12it/s]\n",
      "epoch 100 loss: 2.4376158714294434: 100%|█████████████████████████████████████████████| 63/63 [00:00<00:00, 133.49it/s]\n",
      "epoch 101 loss: 2.11757755279541: 100%|███████████████████████████████████████████████| 63/63 [00:00<00:00, 133.60it/s]\n",
      "epoch 102 loss: 1.9402639865875244: 100%|█████████████████████████████████████████████| 63/63 [00:00<00:00, 132.63it/s]\n",
      "epoch 103 loss: 1.9915359020233154: 100%|█████████████████████████████████████████████| 63/63 [00:00<00:00, 133.75it/s]\n",
      "epoch 104 loss: 1.9241600036621094: 100%|█████████████████████████████████████████████| 63/63 [00:00<00:00, 130.80it/s]\n",
      "epoch 105 loss: 1.7232011556625366: 100%|█████████████████████████████████████████████| 63/63 [00:00<00:00, 134.66it/s]\n",
      "epoch 106 loss: 1.625248670578003: 100%|██████████████████████████████████████████████| 63/63 [00:00<00:00, 130.50it/s]\n",
      "epoch 107 loss: 1.816828966140747: 100%|██████████████████████████████████████████████| 63/63 [00:00<00:00, 132.71it/s]\n",
      "epoch 108 loss: 2.06022572517395: 100%|███████████████████████████████████████████████| 63/63 [00:00<00:00, 132.45it/s]\n",
      "epoch 109 loss: 1.8714460134506226: 100%|█████████████████████████████████████████████| 63/63 [00:00<00:00, 134.62it/s]\n",
      "epoch 110 loss: 1.8081789016723633: 100%|█████████████████████████████████████████████| 63/63 [00:00<00:00, 125.36it/s]\n",
      "epoch 111 loss: 1.7576568126678467: 100%|█████████████████████████████████████████████| 63/63 [00:00<00:00, 133.87it/s]\n",
      "epoch 112 loss: 1.6049089431762695: 100%|█████████████████████████████████████████████| 63/63 [00:00<00:00, 128.01it/s]\n",
      "epoch 113 loss: 1.3975552320480347: 100%|█████████████████████████████████████████████| 63/63 [00:00<00:00, 127.62it/s]\n",
      "epoch 114 loss: 1.425356149673462: 100%|██████████████████████████████████████████████| 63/63 [00:00<00:00, 129.86it/s]\n",
      "epoch 115 loss: 1.4883835315704346: 100%|█████████████████████████████████████████████| 63/63 [00:00<00:00, 132.49it/s]\n",
      "epoch 116 loss: 1.4841965436935425: 100%|█████████████████████████████████████████████| 63/63 [00:00<00:00, 130.17it/s]\n",
      "epoch 117 loss: 1.4530999660491943: 100%|█████████████████████████████████████████████| 63/63 [00:00<00:00, 134.96it/s]\n",
      "epoch 118 loss: 1.3180752992630005: 100%|█████████████████████████████████████████████| 63/63 [00:00<00:00, 131.45it/s]\n",
      "epoch 119 loss: 1.5131462812423706: 100%|█████████████████████████████████████████████| 63/63 [00:00<00:00, 134.99it/s]\n",
      "epoch 120 loss: 1.3154957294464111: 100%|█████████████████████████████████████████████| 63/63 [00:00<00:00, 129.45it/s]\n",
      "epoch 121 loss: 1.2918587923049927: 100%|█████████████████████████████████████████████| 63/63 [00:00<00:00, 135.49it/s]\n",
      "epoch 122 loss: 1.2325621843338013: 100%|█████████████████████████████████████████████| 63/63 [00:00<00:00, 127.97it/s]\n",
      "epoch 123 loss: 1.260004997253418: 100%|██████████████████████████████████████████████| 63/63 [00:00<00:00, 133.96it/s]\n",
      "epoch 124 loss: 1.1307320594787598: 100%|█████████████████████████████████████████████| 63/63 [00:00<00:00, 129.55it/s]\n",
      "epoch 125 loss: 1.0638577938079834: 100%|█████████████████████████████████████████████| 63/63 [00:00<00:00, 132.47it/s]\n",
      "epoch 126 loss: 1.099033236503601: 100%|██████████████████████████████████████████████| 63/63 [00:00<00:00, 130.61it/s]\n",
      "epoch 127 loss: 1.414612054824829: 100%|██████████████████████████████████████████████| 63/63 [00:00<00:00, 133.41it/s]\n",
      "epoch 128 loss: 1.2741777896881104: 100%|█████████████████████████████████████████████| 63/63 [00:00<00:00, 127.48it/s]\n",
      "epoch 129 loss: 1.4525015354156494: 100%|█████████████████████████████████████████████| 63/63 [00:00<00:00, 129.48it/s]\n",
      "epoch 130 loss: 1.2371934652328491: 100%|█████████████████████████████████████████████| 63/63 [00:00<00:00, 136.21it/s]\n",
      "epoch 131 loss: 0.9821169972419739: 100%|█████████████████████████████████████████████| 63/63 [00:00<00:00, 132.16it/s]\n",
      "epoch 132 loss: 0.976392388343811: 100%|██████████████████████████████████████████████| 63/63 [00:00<00:00, 133.95it/s]\n",
      "epoch 133 loss: 0.9899359941482544: 100%|█████████████████████████████████████████████| 63/63 [00:00<00:00, 128.60it/s]\n",
      "epoch 134 loss: 1.3235934972763062: 100%|█████████████████████████████████████████████| 63/63 [00:00<00:00, 136.31it/s]\n",
      "epoch 135 loss: 1.1218788623809814: 100%|█████████████████████████████████████████████| 63/63 [00:00<00:00, 130.91it/s]\n",
      "epoch 136 loss: 0.956277072429657: 100%|██████████████████████████████████████████████| 63/63 [00:00<00:00, 133.63it/s]\n",
      "epoch 137 loss: 0.8439253568649292: 100%|█████████████████████████████████████████████| 63/63 [00:00<00:00, 132.68it/s]\n",
      "epoch 138 loss: 0.8750155568122864: 100%|█████████████████████████████████████████████| 63/63 [00:00<00:00, 127.19it/s]\n",
      "epoch 139 loss: 1.0047589540481567: 100%|█████████████████████████████████████████████| 63/63 [00:00<00:00, 133.04it/s]\n",
      "epoch 140 loss: 0.9310283660888672: 100%|█████████████████████████████████████████████| 63/63 [00:00<00:00, 129.44it/s]\n",
      "epoch 141 loss: 0.7762700915336609: 100%|█████████████████████████████████████████████| 63/63 [00:00<00:00, 134.91it/s]\n",
      "epoch 142 loss: 0.8410525321960449: 100%|█████████████████████████████████████████████| 63/63 [00:00<00:00, 133.05it/s]\n",
      "epoch 143 loss: 1.0359030961990356: 100%|█████████████████████████████████████████████| 63/63 [00:00<00:00, 129.45it/s]\n",
      "epoch 144 loss: 0.8011317253112793: 100%|█████████████████████████████████████████████| 63/63 [00:00<00:00, 134.84it/s]\n",
      "epoch 145 loss: 1.1809957027435303: 100%|█████████████████████████████████████████████| 63/63 [00:00<00:00, 133.27it/s]\n",
      "epoch 146 loss: 0.7873212099075317: 100%|█████████████████████████████████████████████| 63/63 [00:00<00:00, 133.23it/s]\n",
      "epoch 147 loss: 0.8697584867477417: 100%|█████████████████████████████████████████████| 63/63 [00:00<00:00, 133.01it/s]\n",
      "epoch 148 loss: 0.934699535369873: 100%|██████████████████████████████████████████████| 63/63 [00:00<00:00, 131.21it/s]\n",
      "epoch 149 loss: 0.7947691679000854: 100%|█████████████████████████████████████████████| 63/63 [00:00<00:00, 133.04it/s]\n",
      "epoch 150 loss: 0.7574335336685181: 100%|█████████████████████████████████████████████| 63/63 [00:00<00:00, 129.90it/s]\n",
      "epoch 151 loss: 1.1201894283294678: 100%|█████████████████████████████████████████████| 63/63 [00:00<00:00, 132.06it/s]\n",
      "epoch 152 loss: 0.9738104939460754: 100%|█████████████████████████████████████████████| 63/63 [00:00<00:00, 133.76it/s]\n",
      "epoch 153 loss: 1.1713811159133911: 100%|█████████████████████████████████████████████| 63/63 [00:00<00:00, 130.73it/s]\n",
      "epoch 154 loss: 0.9679259061813354: 100%|█████████████████████████████████████████████| 63/63 [00:00<00:00, 126.88it/s]\n",
      "epoch 155 loss: 0.9596496820449829: 100%|█████████████████████████████████████████████| 63/63 [00:00<00:00, 136.12it/s]\n",
      "epoch 156 loss: 0.7720858454704285: 100%|█████████████████████████████████████████████| 63/63 [00:00<00:00, 132.46it/s]\n",
      "epoch 157 loss: 0.6343939900398254: 100%|█████████████████████████████████████████████| 63/63 [00:00<00:00, 131.43it/s]\n",
      "epoch 158 loss: 0.6108651161193848: 100%|█████████████████████████████████████████████| 63/63 [00:00<00:00, 133.05it/s]\n",
      "epoch 159 loss: 0.7172265648841858: 100%|█████████████████████████████████████████████| 63/63 [00:00<00:00, 130.74it/s]\n",
      "epoch 160 loss: 0.5906128287315369: 100%|█████████████████████████████████████████████| 63/63 [00:00<00:00, 128.57it/s]\n",
      "epoch 161 loss: 0.6721156239509583: 100%|█████████████████████████████████████████████| 63/63 [00:00<00:00, 134.57it/s]\n",
      "epoch 162 loss: 0.5284866690635681: 100%|█████████████████████████████████████████████| 63/63 [00:00<00:00, 129.48it/s]\n",
      "epoch 163 loss: 0.5838106274604797: 100%|█████████████████████████████████████████████| 63/63 [00:00<00:00, 130.31it/s]\n",
      "epoch 164 loss: 0.5082234740257263: 100%|█████████████████████████████████████████████| 63/63 [00:00<00:00, 133.17it/s]\n",
      "epoch 165 loss: 0.5366735458374023: 100%|█████████████████████████████████████████████| 63/63 [00:00<00:00, 129.02it/s]\n",
      "epoch 166 loss: 0.485861599445343: 100%|██████████████████████████████████████████████| 63/63 [00:00<00:00, 130.07it/s]\n",
      "epoch 167 loss: 0.5219691395759583: 100%|█████████████████████████████████████████████| 63/63 [00:00<00:00, 133.64it/s]\n",
      "epoch 168 loss: 0.4580300450325012: 100%|█████████████████████████████████████████████| 63/63 [00:00<00:00, 133.17it/s]\n",
      "epoch 169 loss: 0.4842751920223236: 100%|█████████████████████████████████████████████| 63/63 [00:00<00:00, 132.60it/s]\n",
      "epoch 170 loss: 0.5602846145629883: 100%|█████████████████████████████████████████████| 63/63 [00:00<00:00, 129.56it/s]\n",
      "epoch 171 loss: 0.5226671695709229: 100%|█████████████████████████████████████████████| 63/63 [00:00<00:00, 135.83it/s]\n",
      "epoch 172 loss: 0.6667621731758118: 100%|█████████████████████████████████████████████| 63/63 [00:00<00:00, 132.35it/s]\n",
      "epoch 173 loss: 0.4718490242958069: 100%|█████████████████████████████████████████████| 63/63 [00:00<00:00, 131.85it/s]\n",
      "epoch 174 loss: 0.400790274143219: 100%|██████████████████████████████████████████████| 63/63 [00:00<00:00, 133.00it/s]\n",
      "epoch 175 loss: 0.48994940519332886: 100%|████████████████████████████████████████████| 63/63 [00:00<00:00, 129.11it/s]\n",
      "epoch 176 loss: 0.4553622603416443: 100%|█████████████████████████████████████████████| 63/63 [00:00<00:00, 128.93it/s]\n",
      "epoch 177 loss: 0.502872109413147: 100%|██████████████████████████████████████████████| 63/63 [00:00<00:00, 130.32it/s]\n",
      "epoch 178 loss: 0.4958873391151428: 100%|█████████████████████████████████████████████| 63/63 [00:00<00:00, 135.47it/s]\n",
      "epoch 179 loss: 0.7178711295127869: 100%|█████████████████████████████████████████████| 63/63 [00:00<00:00, 132.91it/s]\n",
      "epoch 180 loss: 0.5306560397148132: 100%|█████████████████████████████████████████████| 63/63 [00:00<00:00, 128.37it/s]\n",
      "epoch 181 loss: 0.8123262524604797: 100%|█████████████████████████████████████████████| 63/63 [00:00<00:00, 129.11it/s]\n",
      "epoch 182 loss: 0.5298117399215698: 100%|█████████████████████████████████████████████| 63/63 [00:00<00:00, 133.93it/s]\n",
      "epoch 183 loss: 0.6536736488342285: 100%|█████████████████████████████████████████████| 63/63 [00:00<00:00, 129.94it/s]\n",
      "epoch 184 loss: 0.5872642397880554: 100%|█████████████████████████████████████████████| 63/63 [00:00<00:00, 129.04it/s]\n",
      "epoch 185 loss: 0.4239736497402191: 100%|█████████████████████████████████████████████| 63/63 [00:00<00:00, 130.53it/s]\n",
      "epoch 186 loss: 0.38382431864738464: 100%|████████████████████████████████████████████| 63/63 [00:00<00:00, 132.35it/s]\n",
      "epoch 187 loss: 0.34776049852371216: 100%|████████████████████████████████████████████| 63/63 [00:00<00:00, 128.56it/s]\n",
      "epoch 188 loss: 0.4895545542240143: 100%|█████████████████████████████████████████████| 63/63 [00:00<00:00, 130.41it/s]\n",
      "epoch 189 loss: 0.3429357409477234: 100%|█████████████████████████████████████████████| 63/63 [00:00<00:00, 131.05it/s]\n",
      "epoch 190 loss: 0.43411922454833984: 100%|████████████████████████████████████████████| 63/63 [00:00<00:00, 134.15it/s]\n",
      "epoch 191 loss: 0.3861575722694397: 100%|█████████████████████████████████████████████| 63/63 [00:00<00:00, 128.35it/s]\n",
      "epoch 192 loss: 0.44446754455566406: 100%|████████████████████████████████████████████| 63/63 [00:00<00:00, 128.93it/s]\n",
      "epoch 193 loss: 0.5804327726364136: 100%|█████████████████████████████████████████████| 63/63 [00:00<00:00, 131.00it/s]\n",
      "epoch 194 loss: 0.5583344101905823: 100%|█████████████████████████████████████████████| 63/63 [00:00<00:00, 130.68it/s]\n",
      "epoch 195 loss: 0.36073973774909973: 100%|████████████████████████████████████████████| 63/63 [00:00<00:00, 133.85it/s]\n",
      "epoch 196 loss: 0.4154307246208191: 100%|█████████████████████████████████████████████| 63/63 [00:00<00:00, 131.77it/s]\n",
      "epoch 197 loss: 0.3106730282306671: 100%|█████████████████████████████████████████████| 63/63 [00:00<00:00, 129.25it/s]\n",
      "epoch 198 loss: 0.4833964705467224: 100%|█████████████████████████████████████████████| 63/63 [00:00<00:00, 133.10it/s]\n",
      "epoch 199 loss: 0.35732799768447876: 100%|████████████████████████████████████████████| 63/63 [00:00<00:00, 135.51it/s]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(200):\n",
    "    iterator = tqdm.tqdm(loader)\n",
    "    for data, label in iterator:\n",
    "        optim.zero_grad()\n",
    "\n",
    "        pred = model(torch.tensor(data, dtype=torch.long).to(device))\n",
    "\n",
    "        loss = nn.CrossEntropyLoss()(\n",
    "            pred, torch.tensor(label, dtype=torch.long).to(device)\n",
    "        )\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        iterator.set_description(f\"epoch {epoch} loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1c52c3b-c81c-4d02-8acd-eb59df4f7104",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"lstm.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a37918-faf4-4c43-b437-4232ed7686ad",
   "metadata": {},
   "source": [
    "# 모델 성능 평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c3e8fe9-3b35-4630-99a7-f1dec98cae35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, BOW, string=\"finding an \", strlen=10):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    print(f\"input word: {string}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for p in range(strlen):\n",
    "            words = torch.tensor([BOW[w] for w in string.split()], dtype=torch.long).to(\n",
    "                device\n",
    "            )\n",
    "            # 배치 차원 추가\n",
    "            input_tensor = torch.unsqueeze(words[-2:], dim=0)\n",
    "            output = model(input_tensor)\n",
    "            output_word = torch.argmax(output).cpu().numpy()\n",
    "            string += list(BOW.keys())[output_word]\n",
    "            string += \" \"\n",
    "\n",
    "    print(f\"predicted string: {string}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "54ed23d4-19ea-4a72-9505-592baaa032fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"lstm.pth\", weights_only=True, map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4a7992cf-4d0a-4ca5-9672-30fb5b748766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input word: finding an \n",
      "predicted string: finding an expansive view of a quest or bush no ad insurers \n"
     ]
    }
   ],
   "source": [
    "pred = generate(model, dataset.BOW)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
